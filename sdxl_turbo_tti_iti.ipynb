{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6830b4d-cd2e-4547-b2ba-7c0192ee941a",
   "metadata": {},
   "source": [
    "# Stable Diffusion XL Turbo Text-to-Image and Image-to-Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5062338-01c2-47a1-9362-af4e47b92b5e",
   "metadata": {},
   "source": [
    "[SDXL-Turbo Model Card](https://huggingface.co/stabilityai/sdxl-turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4687ec-c508-4a07-bd02-14f7c7d7d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip3 install -q diffusers transformers accelerate gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f2a116-525a-4d8c-a1c0-2f0a86fd3554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import torch\n",
    "from diffusers import AutoPipelineForText2Image, AutoPipelineForImage2Image\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a3fc1-d329-4142-a28a-6c6b22953d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PyTorch data type\n",
    "torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450abab0-6c63-40fc-a43a-8396f8a49ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metal Performance Shaders (MPS)\n",
    "mps_available = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "\n",
    "if mps_available:\n",
    "    device = torch.device(\"mps\")\n",
    "    torch_dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ceb61-c2d9-4bae-8cc7-606606f1742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set PyTorch device\n",
    "device = torch.device(\n",
    "    \"cuda\" if torch.cuda.is_available() else \"mps\" if mps_available else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43559da7-7863-4d08-bbab-9fd742db1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974fdda5-c34d-41fe-99c6-c9e57edacbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build text-to-image pipeline using Diffusers library\n",
    "pipeline_text2image = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/sdxl-turbo\",\n",
    "    torch_dtype=torch_dtype,\n",
    "    variant=\"fp16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3cbd1-de01-44a3-8b67-0a79b388183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build image-to-image pipeline using from_pipe to avoid consuming additional memory when loading a checkpoint\n",
    "pipeline_image2image = AutoPipelineForImage2Image.from_pipe(pipeline_text2image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d0ded2-abad-47ee-8a80-3d2c4df997f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send pipelines to Torch device\n",
    "pipeline_text2image = pipeline_text2image.to(device=device, dtype=torch_dtype)\n",
    "pipeline_image2image = pipeline_image2image.to(device=device, dtype=torch_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40d16cc-7c89-4af4-bf1b-8f0d66da74a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speed up SDXL Turbo\n",
    "# Compile UNet\n",
    "# pipeline_text2image.unet = torch.compile(pipeline_text2image.unet, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce988b15-82ab-4766-9567-fed313eef395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep default VAE in float32 to avoid costly dtype conversions before and after each generation\n",
    "pipeline_text2image.upcast_vae()\n",
    "pipeline_image2image.upcast_vae()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe45fd3c-48c5-46a3-8917-2fb2aadea760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to generate images from input text and initial image\n",
    "async def generate_image(init_image, prompt, strength, steps, seed=123):\n",
    "    \"\"\"\n",
    "    Generates images from input text only or initial image with input text.\n",
    "    \"\"\"\n",
    "    if init_image is not None:\n",
    "        # init_image = resize_image(init_image)\n",
    "        init_image = init_image.resize(size=(512, 512))\n",
    "        generator = torch.manual_seed(seed)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if int(steps * strength) < 1:\n",
    "            steps = math.ceil(1 / max(0.10, strength))\n",
    "            \n",
    "        out = pipeline_image2image(\n",
    "            prompt=prompt,\n",
    "            image=init_image,\n",
    "            generator=generator,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=0.0,\n",
    "            strength=strength,\n",
    "            width=512,\n",
    "            height=512,\n",
    "            output_type=\"pil\"\n",
    "        )\n",
    "    else:\n",
    "        generator = torch.manual_seed(seed)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        out = pipeline_text2image(\n",
    "            prompt=prompt,\n",
    "            generator=generator,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=0.0,\n",
    "            width=512,\n",
    "            height=512,\n",
    "            output_type=\"pil\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Pipeline processing time: {time.time() - end_time} seconds\")\n",
    "    \n",
    "    nsfw_content_detected = (\n",
    "        out.nsfw_content_detected[0]\n",
    "        if \"nsfw_content_detected\" in out\n",
    "        else False\n",
    "    )\n",
    "    \n",
    "    if nsfw_content_detected:\n",
    "        gr.Warning(\"NSFW content detected\")\n",
    "        return Image.new(mode=\"RGB\", size=(512, 512))\n",
    "    \n",
    "    return out.images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3491a119-e07d-4b3b-8fc6-7733c478bafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gradio application\n",
    "with gr.Blocks() as demo:\n",
    "    init_image_state = gr.State()\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # SDXL Turbo Text-to-Image and Image-to-Image\n",
    "        \"\"\"\n",
    "    )\n",
    "    with gr.Row():\n",
    "        prompt = gr.Textbox(\n",
    "            placeholder=\"Enter a text prompt\",\n",
    "            label=\"Text prompt\",\n",
    "            scale=5\n",
    "        )\n",
    "        btn = gr.Button(\n",
    "            \"Generate\",\n",
    "            scale=1\n",
    "        )\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(\n",
    "                sources=[\"upload\", \"webcam\", \"clipboard\"],\n",
    "                label=\"Initial image\",\n",
    "                type=\"pil\"\n",
    "            )\n",
    "            with gr.Accordion(\"Options\", open=False):\n",
    "                strength = gr.Slider(\n",
    "                    label=\"Strength\",\n",
    "                    value=0.5,\n",
    "                    minimum=0.0,\n",
    "                    maximum=1.0,\n",
    "                    step=0.001\n",
    "                )\n",
    "                steps = gr.Slider(\n",
    "                    label=\"Steps\",\n",
    "                    value=4,\n",
    "                    minimum=1,\n",
    "                    maximum=10,\n",
    "                    step=1\n",
    "                )\n",
    "                seed = gr.Slider(\n",
    "                    randomize=True,\n",
    "                    minimum=0,\n",
    "                    maximum=4294967295,\n",
    "                    label=\"Seed\",\n",
    "                    step=1\n",
    "                )\n",
    "        with gr.Column():\n",
    "            image = gr.Image(type=\"filepath\")\n",
    "    \n",
    "    inputs = [image_input, prompt, strength, steps, seed]\n",
    "    btn.click(fn=generate_image, inputs=inputs, outputs=image, show_progress=False)\n",
    "    prompt.change(fn=generate_image, inputs=inputs, outputs=image, show_progress=False)\n",
    "    steps.change(fn=generate_image, inputs=inputs, outputs=image, show_progress=False)\n",
    "    seed.change(fn=generate_image, inputs=inputs, outputs=image, show_progress=False)\n",
    "    strength.change(fn=generate_image, inputs=inputs, outputs=image, show_progress=False)\n",
    "    image_input.change(\n",
    "        fn=lambda x: x,\n",
    "        inputs=image_input,\n",
    "        outputs=init_image_state,\n",
    "        show_progress=False,\n",
    "        queue=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a160067d-4550-4e71-9617-6e886c88e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a queue with default parameters\n",
    "demo.queue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1e9a5-7aef-49de-8d23-2c7d88dd6d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch Gradio application\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413fac8-1703-451b-b22d-d659bcd88dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close Gradio application\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e009d0-ebfc-42f9-94a3-92479e653a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
